{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap08 - 어텐션(Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Chap08---어텐션(Attention)\" data-toc-modified-id=\"Chap08---어텐션(Attention)-1\">Chap08 - 어텐션(Attention)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Goals\" data-toc-modified-id=\"Goals-1.1\">Goals</a></span></li><li><span><a href=\"#8.1-어텐션의-구조\" data-toc-modified-id=\"8.1-어텐션의-구조-1.2\">8.1 어텐션의 구조</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.1.1-seq2seq의-문제점\" data-toc-modified-id=\"8.1.1-seq2seq의-문제점-1.2.1\">8.1.1 seq2seq의 문제점</a></span></li><li><span><a href=\"#8.1.2-Encoder-개선\" data-toc-modified-id=\"8.1.2-Encoder-개선-1.2.2\">8.1.2 Encoder 개선</a></span></li><li><span><a href=\"#8.1.3-Decoder-개선-①\" data-toc-modified-id=\"8.1.3-Decoder-개선-①-1.2.3\">8.1.3 Decoder 개선 ①</a></span><ul class=\"toc-item\"><li><span><a href=\"#기존의-seq2seq\" data-toc-modified-id=\"기존의-seq2seq-1.2.3.1\">기존의 seq2seq</a></span></li><li><span><a href=\"#개선된-seq2seq\" data-toc-modified-id=\"개선된-seq2seq-1.2.3.2\">개선된 seq2seq</a></span></li><li><span><a href=\"#맥락-벡터-$\\mathbf{c}$를-구하는-과정을-코드로-살펴보기\" data-toc-modified-id=\"맥락-벡터-$\\mathbf{c}$를-구하는-과정을-코드로-살펴보기-1.2.3.3\">맥락 벡터 $\\mathbf{c}$를 구하는 과정을 코드로 살펴보기</a></span></li><li><span><a href=\"#미니배치-처리용-가중합-구현-샘플-코드\" data-toc-modified-id=\"미니배치-처리용-가중합-구현-샘플-코드-1.2.3.4\">미니배치 처리용 가중합 구현 샘플 코드</a></span></li><li><span><a href=\"#가중합의-계산-그래프\" data-toc-modified-id=\"가중합의-계산-그래프-1.2.3.5\">가중합의 계산 그래프</a></span></li><li><span><a href=\"#가중합(WeightSum)-클래스-구현\" data-toc-modified-id=\"가중합(WeightSum)-클래스-구현-1.2.3.6\">가중합(WeightSum) 클래스 구현</a></span></li></ul></li><li><span><a href=\"#8.1.4-Decoder-개선-②\" data-toc-modified-id=\"8.1.4-Decoder-개선-②-1.2.4\">8.1.4 Decoder 개선 ②</a></span><ul class=\"toc-item\"><li><span><a href=\"#attention-weight-샘플코드\" data-toc-modified-id=\"attention-weight-샘플코드-1.2.4.1\">attention weight 샘플코드</a></span></li><li><span><a href=\"#attention-weight의-계산-그래프\" data-toc-modified-id=\"attention-weight의-계산-그래프-1.2.4.2\">attention weight의 계산 그래프</a></span></li><li><span><a href=\"#AttentionWeight-클래스-구현\" data-toc-modified-id=\"AttentionWeight-클래스-구현-1.2.4.3\">AttentionWeight 클래스 구현</a></span></li></ul></li><li><span><a href=\"#8.1.5-Decoder-개선-③\" data-toc-modified-id=\"8.1.5-Decoder-개선-③-1.2.5\">8.1.5 Decoder 개선 ③</a></span><ul class=\"toc-item\"><li><span><a href=\"#Attention-Layer-구성하기\" data-toc-modified-id=\"Attention-Layer-구성하기-1.2.5.1\">Attention Layer 구성하기</a></span></li><li><span><a href=\"#Attention-레이어가-반영된-seq2seq\" data-toc-modified-id=\"Attention-레이어가-반영된-seq2seq-1.2.5.2\">Attention 레이어가 반영된 seq2seq</a></span></li><li><span><a href=\"#TimeAttention-구현\" data-toc-modified-id=\"TimeAttention-구현-1.2.5.3\">TimeAttention 구현</a></span></li></ul></li></ul></li><li><span><a href=\"#8.2-어텐션을-갖춘-seq2seq-구현\" data-toc-modified-id=\"8.2-어텐션을-갖춘-seq2seq-구현-1.3\">8.2 어텐션을 갖춘 seq2seq 구현</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.2.1-Encoder-구현\" data-toc-modified-id=\"8.2.1-Encoder-구현-1.3.1\">8.2.1 Encoder 구현</a></span></li><li><span><a href=\"#8.2.2-Decoder-구현\" data-toc-modified-id=\"8.2.2-Decoder-구현-1.3.2\">8.2.2 Decoder 구현</a></span></li><li><span><a href=\"#8.2.3-seq2seq-구현\" data-toc-modified-id=\"8.2.3-seq2seq-구현-1.3.3\">8.2.3 seq2seq 구현</a></span></li></ul></li><li><span><a href=\"#8.3-어텐션-평가\" data-toc-modified-id=\"8.3-어텐션-평가-1.4\">8.3 어텐션 평가</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.3.1-날짜-형식-변환-문제\" data-toc-modified-id=\"8.3.1-날짜-형식-변환-문제-1.4.1\">8.3.1 날짜 형식 변환 문제</a></span></li><li><span><a href=\"#8.3.2-어텐션을-갖춘-seq2seq의-학습\" data-toc-modified-id=\"8.3.2-어텐션을-갖춘-seq2seq의-학습-1.4.2\">8.3.2 어텐션을 갖춘 seq2seq의 학습</a></span></li><li><span><a href=\"#8.3.3-어텐션-시각화\" data-toc-modified-id=\"8.3.3-어텐션-시각화-1.4.3\">8.3.3 어텐션 시각화</a></span></li></ul></li><li><span><a href=\"#8.4-어텐션에-관한-남은-이야기\" data-toc-modified-id=\"8.4-어텐션에-관한-남은-이야기-1.5\">8.4 어텐션에 관한 남은 이야기</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.4.1-양방향-RNN\" data-toc-modified-id=\"8.4.1-양방향-RNN-1.5.1\">8.4.1 양방향 RNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#TimeBiLSTM-클래스-구현\" data-toc-modified-id=\"TimeBiLSTM-클래스-구현-1.5.1.1\">TimeBiLSTM 클래스 구현</a></span></li></ul></li><li><span><a href=\"#8.4.2-Attention-레이어-사용-방법\" data-toc-modified-id=\"8.4.2-Attention-레이어-사용-방법-1.5.2\">8.4.2 Attention 레이어 사용 방법</a></span></li><li><span><a href=\"#8.4.3-seq2seq-심층화와-skip-연결\" data-toc-modified-id=\"8.4.3-seq2seq-심층화와-skip-연결-1.5.3\">8.4.3 seq2seq 심층화와 skip 연결</a></span></li></ul></li><li><span><a href=\"#8.5-어텐션-응용\" data-toc-modified-id=\"8.5-어텐션-응용-1.6\">8.5 어텐션 응용</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.5.1-구글-신경망-기계-번역(GNMT)\" data-toc-modified-id=\"8.5.1-구글-신경망-기계-번역(GNMT)-1.6.1\">8.5.1 구글 신경망 기계 번역(GNMT)</a></span></li><li><span><a href=\"#8.5.2-Transformer\" data-toc-modified-id=\"8.5.2-Transformer-1.6.2\">8.5.2 Transformer</a></span></li></ul></li><li><span><a href=\"#8.6-정리\" data-toc-modified-id=\"8.6-정리-1.7\">8.6 정리</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- 어텐션이 무엇인가\n",
    "\n",
    "- 어텐션의 구조를 코드 수준에서 이해\n",
    "\n",
    "- 실전 문제에 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 어텐션의 구조\n",
    "\n",
    "- 어텐션 메커니즘을 사용하여 `seq2seq`에서 필요한 정보에만 **'주목'**할 수 있게 된다.\n",
    "\n",
    "- 또한, `seq2seq`가 가지고 있던 문제도 해결할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 seq2seq의 문제점\n",
    "\n",
    "- `seq2seq`에서는 Encoder가 시계열 데이터를 인코딩하고, 이 인코딩된 정보를 Decoder로 전달한다.\n",
    "\n",
    "- 이때 Encoder의 출력은 **'고정 길이 벡터'**였는데, 이 부분에 큰 문제점이 있다.\n",
    "\n",
    "- 고정 길이 벡터는 입력 데이터(문장)의 길이에 관계없이, 항상 **같은** 길이의 벡터로 변환한다.\n",
    "\n",
    "- 그렇기 때문에, 필요한 정보가 벡터에 다 담기지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-1.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 Encoder 개선\n",
    "\n",
    "- 기존의 `seq2seq`는 마지막 timestep의 hidden state $\\mathbf{h}_{t}$만을 Decoder에 전달했다. \n",
    "\n",
    "- 이를 개선하기 위해 Encoder의 <u>출력의 길이를 입력 문장의 길에 맞춰</u> 바꿔주는 것이 좋다.\n",
    "\n",
    "- 아래의 그림과 같이 Encoder의 LSTM 레이어에서 모든 timestep의 hiddent state 벡터를 사용한다.\n",
    "\n",
    "- 각 timestep(각 단어)의 hidden state를 모두 이용하면 입력된 단어와 같은 수의 벡터를 얻을 수 있다. → '하나의 고정 길이 벡터'라는 제약으로부터 해방!\n",
    "\n",
    "> 대붑분의 딥러닝 프레임워크에서는 RNN 계열의 레이어를 초기화할 때, \n",
    "> - 모든 timestep에 대한 hidden state `return`\n",
    "> - 마지막 timestep에서만 hidden state `return`\n",
    "> 을 설정할 수 있다. [Keras](https://keras.io/layers/recurrent/)에서는 `return_sequences=True`를 통해 모든 timestep의 hidden state를 반환 받을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-2.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 주목해야할 부분은 LSTM레이어가 각 timestep마다 출력한 hidden state의 **'내용'**이다.\n",
    "\n",
    "- 각 timestep $t$의 hidden state $\\mathbf{h}_{t}$에는 해당 timestep $t$에 입력된 단어의 정보가 많이 포함되어 있다.\n",
    "\n",
    "- Encoder가 출력하는 $\\mathbf{hs}$행렬은 아래의 그림과 같이 각 단어에 해당하는 벡터들의 집합으로 볼 수 있다.\n",
    "\n",
    "> 예를들어, 위의 그림에서처럼 $t=3$에서 \"고양이\"라는 단어를 입력했을 때, $\\mathbf{h}_{3}$은 \"고양이\"라는 단어의 영향을 가장 크게 받는다. 따라서, $\\mathbf{h}_{3}$은 \"고양이\"라는 단어의 *성분*이 많이 들어간 벡터라고 할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-3.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3 Decoder 개선 ①\n",
    "\n",
    "- Decoder는 아래의 그림과 같이 Encoder의 출력인 $\\mathbf{hs}$를 입력으로 받아 시계열 데이터로 변환하는 작업을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-4.png\" width=\"40%\" height=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기존의 seq2seq\n",
    "\n",
    "- 기존의 `seq2seq`는 Encoder에서 마지막 timestep의 hidden state만을 Decoder의 입력으로 사용했다. \n",
    "\n",
    "- 즉, Encoder의 LSTM 레이어의 **'마지막'** hidden state를 Decoder의 LSTM 레이어의 **'첫'** hidden state로 설정한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-5.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 개선된 seq2seq\n",
    "\n",
    "- 기존의 `seq2seq`에서와는 달리 각 timestep $t$마다 hidden state의 행렬인 $\\mathbf{hs}$ 전부를 활용할 수 있도록 Decoder를 개선한다.\n",
    "\n",
    "> 사람이 문장을 번역할 때,\n",
    "> - *'나 = I', '고양이 = cat'* 이라는 것과 같이 '어떤 단어'에 주목하여 그 단어의 변환을 대응시킬 수 있다. \n",
    "> - 이러한 단어(혹은 문구)의 대응 관계를 나타내는 정보를 **얼라인먼트(alignment)**라 한다.\n",
    "> - 기존의 alignment는 사람이 수작업을 통해 구축했다면, Attention 매커니즘을 통해 alignment를 자동으로 구축할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 Decoder의 목표이며, 이러한 구조를 Attention이라 한다.\n",
    "\n",
    "- 구현하고자 하는 개선된 Decoder의 전체 틀은 아래의 그림과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-6.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 ***'어떤 계산'***이 받는 입력은 두 가지로,\n",
    "    - 하나는 Encoder로 부터 받는 $\\mathbf{hs}$(각 timestep $t$의 hiddent state의 행렬)\n",
    "    - 다른 하나는 Decoder에서 각 timestep별 LSTM의 hidden state 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런다음 필요한 정보만 골라 위쪽의 Affine 레이어의 입력으로 집어 넣는다. \n",
    "\n",
    "- 위의 그림과 같이, 개선된 seq2seq의 Decoder에서 하고자 하는 것은 단어들의 alignment 추출이다.\n",
    "    - Decoder에서 출력된 단어와 대응관계인 단어의 벡터를 $\\mathbf{hs}$에서 찾는다는 의미\n",
    "    - 예를 들어, Decoder가 \"I\"를 출력할 때, $\\mathbf{hs}$에서 \"나\"에 대응하는 벡터를 선택\n",
    "    \n",
    "    \n",
    "- 이러한 **'선택'** 작업을 **'어떤 계산'**으로 수행하겠다는 것이 Attention의 목표이다.\n",
    "\n",
    "- 하지만, 이러한 *'선택'* 작업은 미분을 할 수 없으며, <u>미분이 불가능 하다</u>는 것은 backpropagtion을 통해 **학습을 시킬 수 없다**는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이를 미분 가능한 연산으로 대체하기 위한 방법으로는 '하나를 선택'하는 것이 아니라, **'모든 것을 선택'**하는 것이다.\n",
    "\n",
    "- 그리고 이때, 아래의 그림과 같이 각 단어의 **중요도(기여도)**를 나타내는 *'가중치'* 를 별도로 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-7.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 각 단어의 중요도를 나타내는 '가중치' $\\mathbf{a}$를 이용한다. \n",
    "\n",
    "- $\\mathbf{a}$는 확률분포처럼 각 원소가 0.0 ~ 1.0 사이의 스칼라이며, 모든 원소의 총합은 1이 된다.\n",
    "\n",
    "- 그런 다음, 각 단어의 중요도를 나타내는 가중치 $\\mathbf{a}$와 각 단어의 벡터 $\\mathbf{hs}$로부터 가중합(weighted sum)을 구하여, Attention의 출력값, **맥락 벡터** $\\mathbf{c}$를 얻는다.\n",
    "\n",
    "> 맥락 벡터 $\\mathbf{c}$에는 현 시각(timestep)의 변환을 수행하는 데 필요한 정보가 담겨있으며, 이를 데이터로 부터 학습되게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-8.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 맥락 벡터 $\\mathbf{c}$를 구하는 과정을 코드로 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)  # Encoder의 output인 hs 랜덤하게 생성\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])  # attention distribution\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "# ar = a.reshape(5, 1)  # numpy broadcasting 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8 , 0.8 , 0.8 , 0.8 ],\n",
       "       [0.1 , 0.1 , 0.1 , 0.1 ],\n",
       "       [0.03, 0.03, 0.03, 0.03],\n",
       "       [0.05, 0.05, 0.05, 0.05],\n",
       "       [0.02, 0.02, 0.02, 0.02]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16302473,  1.02837243, -0.52674608, -0.48577415],\n",
       "       [-0.12523668, -0.63852466,  0.32367023, -2.0770472 ],\n",
       "       [ 0.72124699,  1.21389829, -0.74714707,  0.24942445],\n",
       "       [-1.29325626,  1.34291003,  2.05012806, -0.75353779],\n",
       "       [ 2.50445953, -0.15788155, -0.31239195, -1.48325245]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = hs * ar  # point-wise dot\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.sum(t, axis=0)  # 맥락 벡터 c\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12495991,  0.8592503 , -0.31518569, -0.65618325])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 미니배치 처리용 가중합 구현 샘플 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=-1)\n",
    "# ar = a.reshape(N, T, 1)  # 브로드캐스트를 사용하는 경우\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중합의 계산 그래프\n",
    "\n",
    "1. `Repeat`노드를 사용해 $\\mathbf{a}$를 복제하고, \n",
    "\n",
    "2. 이어서 `x` 노드로 원소별 곱을 계산한 다음\n",
    "\n",
    "3. `Sum`노드로 합을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-11.png\" width=\"40%\" height=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중합(WeightSum) 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar= a.reshape(N, T, 1)#.repeat(H, axis=-1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # sum의 역전파 = repeat\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)  # repeat노드에 대한 역전퍄 = sum\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.4 Decoder 개선 ②\n",
    "\n",
    "- 각 단어의 가중치 $\\mathbf{a}$를 구하는 방법을 살표보자. \n",
    "\n",
    "- 아래의 그림은 Decoder의 첫 번째 timestep에서의 LSTM 레이어가 hidden state를 출력하는 부분을 나타낸 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-12.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 Encoder에서의 출력을 $\\mathbf{h}_{s}$라 하고, Decoder의 LSTM의 출력(hidden state)을 $\\mathbf{h}_{t}$라고 정의 했다.\n",
    "\n",
    "- Attention의 목표는 $\\mathbf{h}_{t}$가 $\\mathbf{h}_{s}$의 각 단어 벡터와 얼마나 '비슷한가'를 수치로 나타내는 것이다. \n",
    "\n",
    "- 이를 나타내는 방법으로는 대표적으로 다음과 같이 3가지가 주로 사용된다.\n",
    "    - Basic dot-product attention: $\\mathbf{e}_{i} = \\mathbf{h}_{t}^{\\mathsf{T}} \\mathbf{h}_{s}$\n",
    "    - Multiplicative attention: $\\mathbf{e}_{i} = \\mathbf{h}_{t}^{\\mathsf{T}} \\mathbf{W} \\mathbf{h}_{s}$\n",
    "    - Additive attention: $\\mathbf{e}_{i} = v^{\\mathsf{T}} \\tanh{\\left(\\mathbf{W}_{t} \\mathbf{h}_{t} + \\mathbf{W}_{s}\\mathbf{h}_{s}\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 교재에서는 가장 단순한 방법인 벡터의 '내적'을 이용한다. 두 벡터 $\\mathbf{a} = (a_1, a_2, \\cdots, a_n)$과 $\\mathbf{b} = (b_1, b_2, \\cdots, b_n)$의 내적은 다음과 같이 계산한다.\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\cdots + a_nb_n\n",
    "$$\n",
    "\n",
    "- '내적'의 직관적인 의미는 '두 벡터가 얼마나 같은 방향을 향하고 있는가'이다. 따라서, 두 벡터의 '유사도'를 표현하는 척도로 사용할 수 있다.\n",
    "\n",
    "- 아래의 그림은 벡터의 내적을 이용해 유사도를 산출해내는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-13.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention weight 샘플코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.shape : (10, 5, 4)\n",
      "s.shape : (10, 5)\n",
      "a.shape : (10, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)  # encoder output\n",
    "h = np.random.randn(N, H)  # decoder h_t\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)  # hs와 h_t의 point-wise dot을 위한 처리\n",
    "\n",
    "t = hs * hr  # point-wise dot\n",
    "print(f't.shape : {t.shape}')\n",
    "\n",
    "s = np.sum(t, axis=-1)\n",
    "print(f's.shape : {s.shape}')\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(f'a.shape : {a.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention weight의 계산 그래프\n",
    "\n",
    "<img src=\"./images/fig_8-15.png\" width=\"40%\" height=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AttentionWeight 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap08/attention_layer.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.5 Decoder 개선 ③"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Layer 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-17.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap08/attention_layer.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention 레이어가 반영된 seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-18.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 각 timestep $t$의 `Attention`레이어에는 `Encoder`의 출력인 $\\mathbf{hs}$가 입력된다. \n",
    "\n",
    "- 그리고, Decoder에서 LSTM레이어의 hidden state $\\mathbf{h}_{t}$ 벡터를 `Affine` 레이어에 context 벡터와 concat하여 입력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeAttention 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-20.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Encoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap08/attention_seq2seq.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from seq2seq import Encoder, Seq2seq\n",
    "from attention_layer import TimeAttention\n",
    "\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Decoder 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-21.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()  # Attention 레이어 \n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)  # context vector\n",
    "        out = np.concatenate((c, dec_hs), axis=2)  # context_vector & lstm h_t\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 어텐션 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 날짜 형식 변환 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-22.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 어텐션을 갖춘 seq2seq의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8e42d3a4c07b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     trainer.fit(x_train, t_train, max_epoch=1,\n\u001b[1;32m---> 44\u001b[1;33m                 batch_size=batch_size, max_grad=max_grad, eval_interval=150)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mcorrect_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Gina\\DLFromScratch2\\common\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m# 기울기 구해 매개변수 갱신\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 공유된 가중치를 하나로 모음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmax_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Gina\\DLFromScratch2\\Chap08-Attention\\seq2seq.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Gina\\DLFromScratch2\\Chap08-Attention\\attention_seq2seq.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dscore)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mdc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddec_hs0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mdenc_hs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddec_hs1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mddec_hs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddec_hs0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mddec_hs1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddec_hs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Gina\\DLFromScratch2\\Chap08-Attention\\attention_layer.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mdhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mdhs_enc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdhs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mdhs_dec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Gina\\DLFromScratch2\\Chap08-Attention\\attention_layer.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mdhs0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_sum_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mdhs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_weight_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mdhs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdhs0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdhs1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Gina\\DLFromScratch2\\Chap08-Attention\\attention_layer.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, da)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mdhs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mdhr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad, eval_interval=150)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3 어텐션 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (59,16) into shape (60,16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-80d36767718d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttentionSeq2seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordvec_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0m_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Gina\\DLFromScratch2\\common\\base_model.py\u001b[0m in \u001b[0;36mload_params\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (59,16) into shape (60,16)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(1984)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 어텐션에 관한 남은 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 양방향 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-30.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bidirectional RNN(LSTM, GRU 등)에서는 정방향 LSTM 레이어에 *역방향* 으로 처리하는 LSTM 레이어를 추가한 형태를 말한다.\n",
    "\n",
    "- 그리고, 각 timestep $t$에서 정방향 & 역방향 LSTM의 hidden state $\\mathbf{h}_{t}$를 연결(concat, 또는 sum, average 등)시킨 벡터를 최종 $\\mathbf{h}_{t}$로 만든다.\n",
    "\n",
    "- 양방향으로 처리함으로써, 각 단어에 대응하는 hidden state 벡터에는 forward, backward 방양으로부터의 정보를 집약할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeBiLSTM 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/time_layers.py\n",
    "\n",
    "class TimeBiLSTM:\n",
    "    def __init__(self, Wx1, Wh1, b1,\n",
    "                 Wx2, Wh2, b2, stateful=False):\n",
    "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
    "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
    "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
    "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
    "\n",
    "    def forward(self, xs):\n",
    "        o1 = self.forward_lstm.forward(xs)\n",
    "        o2 = self.backward_lstm.forward(xs[:, ::-1])  # backward를 위해 입력데이터 반전\n",
    "        o2 = o2[:, ::-1]\n",
    "\n",
    "        out = np.concatenate((o1, o2), axis=2)  # forward, backward concat\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        H = dhs.shape[2] // 2\n",
    "        do1 = dhs[:, :, :H]\n",
    "        do2 = dhs[:, :, H:]\n",
    "\n",
    "        dxs1 = self.forward_lstm.backward(do1)\n",
    "        do2 = do2[:, ::-1]\n",
    "        dxs2 = self.backward_lstm.backward(do2)\n",
    "        dxs2 = dxs2[:, ::-1]\n",
    "        dxs = dxs1 + dxs2\n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2 Attention 레이어 사용 방법\n",
    "\n",
    "- Attention 레이어는 다양하게 조합되어 사용할 수 있다. \n",
    "\n",
    "- 다만, Attention 레이어를 조합하였을 때, 정확도에 대한 영향은 직접 데이터를 사용해 검증해보지 않으면 모른다. \n",
    "\n",
    "- 아래의 그림(출처: [CN Yah 블로그](http://cnyah.com/2017/08/01/attention-variants/))과 같이 Bahdanau Attention 메커니즘의 경우에는 LSTM 레이어가 context 벡터의 정보를 이용할 수 있도록 구성하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/attention-mechanisms.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.3 seq2seq 심층화와 skip 연결\n",
    "\n",
    "- 번역 등 현실에서의 애플리케이션들은 풀어야 할 문제가 훨씬 복잡하다.\n",
    "\n",
    "- 이럴 경우에 층을 깊게 쌓은 seq2seq를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-33.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 층을 깊게 쌓을 때 사용되는 중요한 기법 중에 **skip connection**이 있다(residual connection 또는 short-cut).\n",
    "\n",
    "- skip connection에서는 2개의 출력이 하나로 **'더해'**지기 때문에 역전파 시 기울기가 **'그대로 흘려'** 보낸다.\n",
    "\n",
    "- 따라서, 층이 깊어져도 기울기 소실(또는 폭발)이 되지 않고 전파되어, 좋은 학습을 기대할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-34.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 어텐션 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1 구글 신경망 기계 번역(GNMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-35.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/transformer.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 정리\n",
    "\n",
    "- 어텐션은 두 시계열 데이터 사이의 대응 관계를 데이터로부터 학습한다.\n",
    "\n",
    "- 어텐션에서는 벡터의 내적을 사용해 벡터 사이의 유사도를 구하고, 그 유사도를 이용한 가중합 벡터가 어텐션의 출력이 된다.\n",
    "\n",
    "- 어텐션에서 사용하는 연산은 미분 가능하기 때문에 오차역전파법으로 학습할 수 있다.\n",
    "\n",
    "- 어텐션이 산출하는 가중치(확률)을 시각화하면 입출력의 대응 관계를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
